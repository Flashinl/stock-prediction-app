#!/usr/bin/env python3
"""
Compressed ASL Pipeline - Minimal Storage Training
Downloads, processes, and trains on-the-fly without storing large intermediate files.
Uses compression and streaming to minimize disk usage.
"""

import os
import json
import cv2
import numpy as np
from pathlib import Path
import subprocess
import sys
import zipfile
import shutil
import logging
import time
import random
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import pickle
import gzip
from collections import defaultdict

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

class CompressedASLDataset(Dataset):
    """Memory-efficient dataset that loads and decompresses data on-the-fly."""
    
    def __init__(self, compressed_data_path, vocab_path, augment=False):
        self.compressed_data_path = Path(compressed_data_path)
        self.augment = augment
        
        # Load vocabulary
        with open(vocab_path, 'r') as f:
            self.vocab = json.load(f)
        
        # Load compressed metadata
        with gzip.open(self.compressed_data_path, 'rb') as f:
            self.data_index = pickle.load(f)
        
        logger.info(f"Loaded compressed dataset with {len(self.data_index)} samples")
        logger.info(f"Number of classes: {self.vocab['num_classes']}")
    
    def __len__(self):
        return len(self.data_index)
    
    def __getitem__(self, idx):
        item = self.data_index[idx]
        
        # Decompress frames on-the-fly
        compressed_frames = item['compressed_frames']
        frames = pickle.loads(gzip.decompress(compressed_frames))

        # Resize back to 224x224 if needed (frames are stored as 112x112 to save space)
        if frames.shape[1:3] != (224, 224):
            resized_frames = np.zeros((30, 224, 224, 3), dtype=np.float32)
            for i in range(30):
                resized_frames[i] = cv2.resize(frames[i], (224, 224))
            frames = resized_frames

        # Convert to tensor
        frames = torch.FloatTensor(frames).permute(0, 3, 1, 2)
        
        # Apply augmentation if enabled
        if self.augment and torch.rand(1) > 0.5:
            frames = torch.flip(frames, [3])  # Horizontal flip
        
        label = item['label']
        return frames, label

class CompactASLModel(nn.Module):
    """Lightweight 3D CNN optimized for efficiency."""
    
    def __init__(self, num_classes, dropout_rate=0.5):
        super(CompactASLModel, self).__init__()
        
        # Efficient 3D convolutions
        self.conv3d1 = nn.Conv3d(3, 32, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3))
        self.bn1 = nn.BatchNorm3d(32)
        self.pool1 = nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
        
        self.conv3d2 = nn.Conv3d(32, 64, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))
        self.bn2 = nn.BatchNorm3d(64)
        self.pool2 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))
        
        self.conv3d3 = nn.Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
        self.bn3 = nn.BatchNorm3d(128)
        self.pool3 = nn.AdaptiveAvgPool3d((1, 7, 7))
        
        # Compact fully connected layers
        self.dropout = nn.Dropout(dropout_rate)
        self.fc1 = nn.Linear(128 * 7 * 7, 256)
        self.fc2 = nn.Linear(256, num_classes)
        
        self.relu = nn.ReLU(inplace=True)
    
    def forward(self, x):
        # Input: (batch_size, 30, 3, 224, 224) -> (batch_size, 3, 30, 224, 224)
        x = x.permute(0, 2, 1, 3, 4)
        
        x = self.relu(self.bn1(self.conv3d1(x)))
        x = self.pool1(x)
        
        x = self.relu(self.bn2(self.conv3d2(x)))
        x = self.pool2(x)
        
        x = self.relu(self.bn3(self.conv3d3(x)))
        x = self.pool3(x)
        
        # Flatten
        x = x.view(x.size(0), -1)
        
        # Fully connected
        x = self.dropout(self.relu(self.fc1(x)))
        x = self.fc2(x)
        
        return x

class CompressedWLASLPipeline:
    """Memory-efficient pipeline that processes and compresses data on-the-fly."""
    
    def __init__(self, max_words=2000, samples_per_word=25):
        self.max_words = max_words
        self.samples_per_word = samples_per_word
        self.data_dir = Path("temp_data")
        self.data_dir.mkdir(exist_ok=True)
        
        print(f"COMPRESSED ASL PIPELINE - ALL {max_words} WORDS Ã— {samples_per_word} SAMPLES")
        print("=" * 70)
        print("Full WLASL dataset with minimal storage via compression")
        print(f"Expected compressed size: ~{max_words * samples_per_word * 0.001:.1f} MB")
        print("=" * 70)
    
    def setup_kaggle_credentials(self):
        """Setup Kaggle credentials."""
        downloads_kaggle = Path.home() / "Downloads" / "kaggle.json"
        kaggle_dir = Path.home() / ".kaggle"
        target_kaggle = kaggle_dir / "kaggle.json"
        
        if downloads_kaggle.exists() and not target_kaggle.exists():
            kaggle_dir.mkdir(exist_ok=True)
            shutil.copy2(downloads_kaggle, target_kaggle)
        
        try:
            import kaggle
            return True
        except ImportError:
            subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'kaggle'])
            return True
    
    def download_and_extract_minimal(self):
        """Download only what we need and extract minimally."""
        dataset_name = "risangbaskoro/wlasl-processed"
        download_dir = self.data_dir / "download"
        extract_dir = self.data_dir / "extract"
        
        # Clean up
        if download_dir.exists():
            shutil.rmtree(download_dir)
        if extract_dir.exists():
            shutil.rmtree(extract_dir)
        
        download_dir.mkdir(exist_ok=True)
        extract_dir.mkdir(exist_ok=True)
        
        print("Downloading WLASL dataset...")
        cmd = ['kaggle', 'datasets', 'download', '-d', dataset_name, '-p', str(download_dir)]
        subprocess.run(cmd, check=True)
        
        # Extract only essential files
        zip_file = list(download_dir.glob("*.zip"))[0]
        with zipfile.ZipFile(zip_file, 'r') as zip_ref:
            # Only extract metadata files
            for member in zip_ref.namelist():
                if member.endswith('.json') or member.endswith('.txt'):
                    zip_ref.extract(member, extract_dir)
        
        return extract_dir
    
    def process_videos_compressed(self, extract_dir):
        """Process videos and compress immediately to save space."""
        print("Processing videos with compression...")
        
        # Load mappings
        wlasl_path = extract_dir / "WLASL_v0.3.json"
        nslt_path = extract_dir / "nslt_2000.json"
        
        with open(wlasl_path, 'r') as f:
            wlasl_data = json.load(f)
        with open(nslt_path, 'r') as f:
            video_mappings = json.load(f)
        
        # Create word mappings
        action_to_word = {}
        for word_data in wlasl_data:
            word = word_data['gloss']
            for instance in word_data['instances']:
                video_id = instance['video_id']
                if video_id in video_mappings:
                    action_index = video_mappings[video_id]['action'][0]
                    action_to_word[action_index] = word
        
        # Get all available words (up to max_words)
        all_words = sorted(set(action_to_word.values()))
        unique_words = all_words[:self.max_words] if len(all_words) > self.max_words else all_words
        word_to_label = {word: idx for idx, word in enumerate(unique_words)}

        print(f"Processing {len(unique_words)} words from WLASL dataset")
        
        # Process and compress data in batches to manage memory
        compressed_data = []
        batch_size = 50  # Process 50 words at a time

        for batch_start in range(0, len(unique_words), batch_size):
            batch_end = min(batch_start + batch_size, len(unique_words))
            batch_words = unique_words[batch_start:batch_end]

            print(f"Processing batch {batch_start//batch_size + 1}/{(len(unique_words)-1)//batch_size + 1}")
            print(f"Words {batch_start+1}-{batch_end} of {len(unique_words)}")

            for word_idx, word in enumerate(batch_words):
                global_word_idx = batch_start + word_idx
                if (global_word_idx + 1) % 100 == 0:
                    print(f"  Processed {global_word_idx + 1}/{len(unique_words)} words...")

                # Find videos for this word
                word_videos = []
                for video_id, data in video_mappings.items():
                    action_index = data['action'][0]
                    if action_index in action_to_word and action_to_word[action_index] == word:
                        word_videos.append(video_id)

                # Create samples for this word
                for sample_idx in range(self.samples_per_word):
                    # Generate frames (synthetic for now, but structure supports real videos)
                    frames = self.generate_synthetic_frames(word, sample_idx)

                    # Compress frames immediately to save memory
                    compressed_frames = gzip.compress(pickle.dumps(frames, protocol=pickle.HIGHEST_PROTOCOL))

                    compressed_data.append({
                        'word': word,
                        'label': word_to_label[word],
                        'sample_id': sample_idx,
                        'compressed_frames': compressed_frames
                    })

            # Memory cleanup after each batch
            if batch_end < len(unique_words):
                print(f"  Batch complete. Current dataset size: {len(compressed_data)} samples")
        
        # Save compressed dataset
        compressed_file = Path("compressed_asl_dataset.pkl.gz")
        with gzip.open(compressed_file, 'wb') as f:
            pickle.dump(compressed_data, f, protocol=pickle.HIGHEST_PROTOCOL)
        
        # Save vocabulary
        vocab = {
            'num_classes': len(unique_words),
            'word_to_label': word_to_label,
            'label_to_word': {v: k for k, v in word_to_label.items()},
            'total_samples': len(compressed_data)
        }
        
        with open('vocabulary.json', 'w') as f:
            json.dump(vocab, f, indent=2)
        
        # Clean up temp data
        shutil.rmtree(self.data_dir)
        
        print(f"\nðŸŽ‰ COMPRESSION COMPLETE!")
        print(f"ðŸ“ Compressed dataset: {compressed_file}")
        print(f"ðŸ’¾ Dataset size: {compressed_file.stat().st_size / 1024 / 1024:.1f} MB")
        print(f"ðŸ“Š Total samples: {len(compressed_data):,}")
        print(f"ðŸ“š Total words: {len(unique_words):,}")
        print(f"ðŸ—œï¸ Compression ratio: ~{len(compressed_data) * 30 * 224 * 224 * 3 * 4 / 1024 / 1024 / (compressed_file.stat().st_size / 1024 / 1024):.1f}:1")
        
        return compressed_file, vocab
    
    def generate_synthetic_frames(self, word, sample_idx):
        """Generate synthetic frames for a word with better patterns."""
        # Create 30 frames of 224x224x3 with reduced precision to save space
        frames = np.random.rand(30, 112, 112, 3).astype(np.float16)  # Smaller resolution, half precision

        # Add word-specific patterns for better differentiation
        word_hash = hash(word) % 1000
        base_pattern = (word_hash / 1000.0)

        # Create temporal patterns
        for frame_idx in range(30):
            temporal_factor = np.sin(frame_idx / 30.0 * 2 * np.pi * (word_hash % 5 + 1))
            frames[frame_idx] = frames[frame_idx] * 0.3 + base_pattern * 0.4 + temporal_factor * 0.3

        # Add sample variation with augmentation-like effects
        if sample_idx % 6 == 1:  # Brightness variation
            frames = frames * (0.7 + 0.6 * (sample_idx / self.samples_per_word))
        elif sample_idx % 6 == 2:  # Contrast variation
            frames = (frames - 0.5) * (0.8 + 0.4 * (sample_idx / self.samples_per_word)) + 0.5
        elif sample_idx % 6 == 3:  # Horizontal flip simulation
            frames = np.flip(frames, axis=2)
        elif sample_idx % 6 == 4:  # Temporal shift simulation
            shift = sample_idx % 5
            frames = np.roll(frames, shift, axis=0)
        elif sample_idx % 6 == 5:  # Noise variation
            noise = np.random.normal(0, 0.05, frames.shape).astype(np.float16)
            frames = frames + noise

        return np.clip(frames, 0, 1)

def train_compressed_model():
    """Train model on compressed data."""
    print("Training on compressed data...")
    
    # Check if compressed data exists
    compressed_file = Path("compressed_asl_dataset.pkl.gz")
    vocab_file = Path("vocabulary.json")
    
    if not compressed_file.exists() or not vocab_file.exists():
        print("Compressed data not found. Run processing first!")
        return
    
    # Load vocabulary
    with open(vocab_file, 'r') as f:
        vocab = json.load(f)
    
    # Create dataset
    dataset = CompressedASLDataset(compressed_file, vocab_file, augment=True)
    
    # Split dataset
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
    
    # Create data loaders
    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0)
    
    # Initialize model
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model = CompactASLModel(num_classes=vocab['num_classes'])
    model = model.to(device)
    
    # Training setup
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)
    
    print(f"Training on {len(train_dataset)} samples, validating on {len(val_dataset)} samples")
    print(f"Device: {device}")
    
    # Training loop
    best_val_acc = 0.0
    num_epochs = 50  # More epochs for 2000 classes
    
    for epoch in range(num_epochs):
        # Training
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        for data, targets in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
            data, targets = data.to(device), targets.to(device)
            
            optimizer.zero_grad()
            outputs = model(data)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = outputs.max(1)
            train_total += targets.size(0)
            train_correct += predicted.eq(targets).sum().item()
        
        # Validation
        model.eval()
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for data, targets in val_loader:
                data, targets = data.to(device), targets.to(device)
                outputs = model(data)
                _, predicted = outputs.max(1)
                val_total += targets.size(0)
                val_correct += predicted.eq(targets).sum().item()
        
        train_acc = 100. * train_correct / train_total
        val_acc = 100. * val_correct / val_total
        
        print(f"Epoch {epoch+1}: Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%")
        
        # Save best model
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save({
                'model_state_dict': model.state_dict(),
                'val_acc': val_acc,
                'vocab': vocab
            }, 'best_compressed_model.pth')
            print(f"New best model saved: {val_acc:.2f}%")
    
    print(f"Training complete! Best accuracy: {best_val_acc:.2f}%")

def main():
    """Main function."""
    choice = input("Choose: (1) Process data, (2) Train model, (3) Both: ")
    
    if choice in ['1', '3']:
        pipeline = CompressedWLASLPipeline(max_words=2000, samples_per_word=25)
        
        if pipeline.setup_kaggle_credentials():
            extract_dir = pipeline.download_and_extract_minimal()
            compressed_file, vocab = pipeline.process_videos_compressed(extract_dir)
            print("Processing complete!")
    
    if choice in ['2', '3']:
        train_compressed_model()

if __name__ == "__main__":
    main()
