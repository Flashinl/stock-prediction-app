#!/usr/bin/env python3
"""
Optimized Model Training - Targeting 80%+ Accuracy
Improved target generation and feature engineering
"""

import logging
import sys
import os
import json
import pandas as pd
import numpy as np
from datetime import datetime, timezone
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import joblib
import re
import random

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class OptimizedModelTrainer:
    def __init__(self):
        self.model = None
        self.scaler = StandardScaler()
        self.valid_symbols = set()
        
        # Set random seed for reproducibility
        random.seed(42)
        np.random.seed(42)
        
        # Invalid symbol patterns
        self.invalid_patterns = [
            r'^\$', r'_[A-Z]$', r'-[A-Z]+$', r'[^A-Z0-9]', r'^[0-9]'
        ]
        
    def is_valid_symbol(self, symbol):
        """Check if a stock symbol is valid"""
        if not symbol or len(symbol) < 1 or len(symbol) > 5:
            return False
        for pattern in self.invalid_patterns:
            if re.search(pattern, symbol):
                return False
        return True
    
    def load_optimized_data(self):
        """Load optimized training data with better target generation"""
        logger.info("Loading optimized training data...")
        
        all_stock_data = []
        target_samples = {'BUY': 0, 'HOLD': 0, 'SELL': 0}
        max_per_target = 10000  # 10K samples per target = 30K total (more balanced)
        
        stocks_dir = 'kaggle_data/borismarjanovic_price-volume-data-for-all-us-stocks-etfs/Stocks'
        if not os.path.exists(stocks_dir):
            logger.error("Kaggle data directory not found")
            return []
        
        stock_files = [f for f in os.listdir(stocks_dir) if f.endswith('.txt')]
        random.shuffle(stock_files)
        
        logger.info(f"Processing stocks for optimized dataset...")
        
        for i, stock_file in enumerate(stock_files):
            if i % 500 == 0:
                logger.info(f"Processed {i} stocks, collected {len(all_stock_data)} samples")
                logger.info(f"Current distribution: {target_samples}")
            
            # Stop if we have enough samples
            if all(count >= max_per_target for count in target_samples.values()):
                break
            
            symbol = stock_file.replace('.us.txt', '').upper()
            if not self.is_valid_symbol(symbol):
                continue
            
            try:
                stock_samples = self._process_stock_optimized(
                    os.path.join(stocks_dir, stock_file), symbol
                )
                
                if stock_samples:
                    # Add samples while maintaining balance
                    for sample in stock_samples:
                        target = sample['target']
                        if target_samples[target] < max_per_target:
                            all_stock_data.append(sample)
                            target_samples[target] += 1
                            
                    self.valid_symbols.add(symbol)
                    
            except Exception as e:
                logger.debug(f"Error processing {symbol}: {e}")
                continue
        
        logger.info(f"Final dataset: {len(all_stock_data)} samples from {len(self.valid_symbols)} stocks")
        logger.info(f"Target distribution: {target_samples}")
        return all_stock_data
    
    def _process_stock_optimized(self, file_path, symbol):
        """Process stock with optimized target generation"""
        try:
            df = pd.read_csv(file_path)
            if len(df) < 150:
                return None
            
            df['Date'] = pd.to_datetime(df['Date'])
            df = df.sort_values('Date')
            
            samples = []
            window_size = 45  # Shorter window for more samples
            prediction_horizon = 20  # Shorter prediction horizon
            step_size = 15  # More frequent sampling
            
            for i in range(window_size, len(df) - prediction_horizon, step_size):
                current_window = df.iloc[i-window_size:i]
                future_window = df.iloc[i:i+prediction_horizon]
                
                if len(current_window) < window_size or len(future_window) < prediction_horizon:
                    continue
                
                features = self._calculate_optimized_features(current_window)
                if features is None:
                    continue
                
                # Optimized target generation - more realistic thresholds
                current_price = current_window['Close'].iloc[-1]
                future_price = future_window['Close'].iloc[-1]
                future_return = (future_price - current_price) / current_price
                
                # More achievable thresholds based on our data analysis
                if future_return > 0.06:  # 6% gain threshold (more realistic)
                    target = 'BUY'
                elif future_return < -0.06:  # 6% loss threshold
                    target = 'SELL'
                else:
                    target = 'HOLD'
                
                samples.append({
                    'symbol': symbol,
                    'features': features,
                    'target': target,
                    'future_return': future_return
                })
            
            return samples[:8] if samples else None  # Max 8 samples per stock
            
        except Exception as e:
            logger.debug(f"Error processing {symbol}: {e}")
            return None
    
    def _calculate_optimized_features(self, window_data):
        """Calculate optimized features for better prediction"""
        try:
            prices = window_data['Close'].values
            volumes = window_data['Volume'].values
            highs = window_data['High'].values
            lows = window_data['Low'].values
            
            if len(prices) < 20:
                return None
            
            # Basic price features
            current_price = prices[-1]
            returns_1d = (prices[-1] - prices[-2]) / prices[-2] if len(prices) > 1 else 0
            returns_3d = (prices[-1] - prices[-4]) / prices[-4] if len(prices) > 3 else 0
            returns_7d = (prices[-1] - prices[-8]) / prices[-8] if len(prices) > 7 else 0
            returns_14d = (prices[-1] - prices[-15]) / prices[-15] if len(prices) > 14 else 0
            
            # Moving averages
            ma_5 = np.mean(prices[-5:])
            ma_10 = np.mean(prices[-10:])
            ma_20 = np.mean(prices[-20:])
            
            # Technical indicators
            rsi = self._calculate_rsi(prices)
            
            # Volatility measures
            volatility_5d = np.std(prices[-5:]) / np.mean(prices[-5:])
            volatility_20d = np.std(prices[-20:]) / np.mean(prices[-20:])
            
            # Volume analysis (handle zero volume)
            avg_volume = np.mean(volumes)
            volume_ratio = volumes[-1] / avg_volume if avg_volume > 0 else 1
            volume_trend = np.mean(volumes[-5:]) / np.mean(volumes[-15:]) if np.mean(volumes[-15:]) > 0 else 1
            
            # Price position and range
            high_20d = np.max(highs[-20:])
            low_20d = np.min(lows[-20:])
            price_range = high_20d - low_20d
            price_position = (current_price - low_20d) / price_range if price_range > 0 else 0.5
            
            # Trend analysis
            price_vs_ma5 = current_price / ma_5 - 1
            price_vs_ma10 = current_price / ma_10 - 1
            price_vs_ma20 = current_price / ma_20 - 1
            ma_alignment = 1 if ma_5 > ma_10 > ma_20 else 0
            
            # Momentum indicators
            momentum_3d = np.mean(prices[-3:]) / np.mean(prices[-6:-3]) - 1 if len(prices) >= 6 else 0
            momentum_7d = np.mean(prices[-7:]) / np.mean(prices[-14:-7]) - 1 if len(prices) >= 14 else 0
            
            # Price acceleration
            recent_slope = (prices[-1] - prices[-5]) / 4 if len(prices) >= 5 else 0
            older_slope = (prices[-6] - prices[-10]) / 4 if len(prices) >= 10 else 0
            acceleration = recent_slope - older_slope
            
            # Support/Resistance levels
            support_level = np.min(lows[-10:]) if len(lows) >= 10 else current_price
            resistance_level = np.max(highs[-10:]) if len(highs) >= 10 else current_price
            support_distance = (current_price - support_level) / current_price if current_price > 0 else 0
            resistance_distance = (resistance_level - current_price) / current_price if current_price > 0 else 0
            
            return [
                current_price,
                returns_1d,
                returns_3d,
                returns_7d,
                returns_14d,
                ma_5,
                ma_10,
                ma_20,
                rsi,
                volatility_5d,
                volatility_20d,
                volume_ratio,
                volume_trend,
                price_position,
                price_vs_ma5,
                price_vs_ma10,
                price_vs_ma20,
                ma_alignment,
                momentum_3d,
                momentum_7d,
                acceleration,
                support_distance,
                resistance_distance
            ]
            
        except Exception as e:
            logger.debug(f"Error calculating features: {e}")
            return None
    
    def _calculate_rsi(self, prices, period=14):
        """Calculate RSI indicator"""
        try:
            if len(prices) < period + 1:
                return 50
                
            deltas = np.diff(prices)
            gains = np.where(deltas > 0, deltas, 0)
            losses = np.where(deltas < 0, -deltas, 0)
            
            avg_gain = np.mean(gains[-period:])
            avg_loss = np.mean(losses[-period:])
            
            if avg_loss == 0:
                return 100
                
            rs = avg_gain / avg_loss
            rsi = 100 - (100 / (1 + rs))
            return rsi
            
        except:
            return 50

    def train_optimized_model(self, stock_data_list):
        """Train optimized model with advanced techniques"""
        logger.info("Training optimized model...")

        try:
            # Prepare data
            X = np.array([data['features'] for data in stock_data_list])
            y = np.array([data['target'] for data in stock_data_list])

            logger.info(f"Training with {len(X)} samples")
            unique, counts = np.unique(y, return_counts=True)
            target_dist = dict(zip(unique, counts))
            logger.info(f"Target distribution: {target_dist}")

            # Split data
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )

            # Scale features
            X_train_scaled = self.scaler.fit_transform(X_train)
            X_test_scaled = self.scaler.transform(X_test)

            # Train optimized model with better parameters
            self.model = RandomForestClassifier(
                n_estimators=500,  # More trees
                max_depth=20,      # Deeper trees
                min_samples_split=5,
                min_samples_leaf=3,
                max_features='sqrt',
                random_state=42,
                n_jobs=-1,
                class_weight='balanced_subsample'  # Better class balancing
            )

            # Cross-validation
            cv_scores = cross_val_score(self.model, X_train_scaled, y_train, cv=5)
            logger.info(f"Cross-validation scores: {cv_scores}")
            logger.info(f"Mean CV accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")

            # Train final model
            self.model.fit(X_train_scaled, y_train)

            # Evaluate
            y_pred = self.model.predict(X_test_scaled)
            accuracy = accuracy_score(y_test, y_pred)

            logger.info(f"Test accuracy: {accuracy:.4f}")
            logger.info("Classification Report:")
            logger.info(f"\n{classification_report(y_test, y_pred)}")

            # Confusion matrix
            cm = confusion_matrix(y_test, y_pred)
            logger.info(f"Confusion Matrix:\n{cm}")

            # Feature importance
            feature_names = [
                'current_price', 'returns_1d', 'returns_3d', 'returns_7d', 'returns_14d',
                'ma_5', 'ma_10', 'ma_20', 'rsi', 'volatility_5d', 'volatility_20d',
                'volume_ratio', 'volume_trend', 'price_position', 'price_vs_ma5',
                'price_vs_ma10', 'price_vs_ma20', 'ma_alignment', 'momentum_3d',
                'momentum_7d', 'acceleration', 'support_distance', 'resistance_distance'
            ]

            importances = self.model.feature_importances_
            feature_importance = list(zip(feature_names, importances))
            feature_importance.sort(key=lambda x: x[1], reverse=True)

            logger.info("Top 10 Feature Importances:")
            for name, importance in feature_importance[:10]:
                logger.info(f"  {name}: {importance:.4f}")

            # Save model if accuracy is good
            if accuracy >= 0.70:  # 70% threshold
                os.makedirs('models', exist_ok=True)
                joblib.dump(self.model, 'models/optimized_stock_model.joblib')
                joblib.dump(self.scaler, 'models/optimized_stock_scaler.joblib')

                with open('models/optimized_features.json', 'w') as f:
                    json.dump(feature_names, f)

                logger.info("Optimized model saved successfully!")

            return accuracy >= 0.80, accuracy  # Return if we hit 80%+ target

        except Exception as e:
            logger.error(f"Error during training: {e}")
            return False, 0

    def save_optimized_report(self, stock_data_list, accuracy):
        """Save optimized training report"""
        try:
            target_counts = {}
            return_stats = {'BUY': [], 'HOLD': [], 'SELL': []}

            for data in stock_data_list:
                target = data['target']
                target_counts[target] = target_counts.get(target, 0) + 1
                if 'future_return' in data:
                    return_stats[target].append(data['future_return'])

            return_summary = {}
            for target, returns in return_stats.items():
                if returns:
                    return_summary[target] = {
                        'count': len(returns),
                        'mean_return': np.mean(returns),
                        'std_return': np.std(returns),
                        'min_return': np.min(returns),
                        'max_return': np.max(returns)
                    }

            report = {
                'timestamp': datetime.now(timezone.utc).isoformat(),
                'model_type': 'Optimized Random Forest',
                'total_training_samples': len(stock_data_list),
                'unique_stocks': len(self.valid_symbols),
                'target_distribution': target_counts,
                'return_statistics': return_summary,
                'model_accuracy': accuracy,
                'accuracy_threshold_met': accuracy >= 0.80,
                'feature_count': 23,
                'improvements': [
                    'Reduced target thresholds to 6% (more realistic)',
                    'Added 23 advanced technical features',
                    'Balanced dataset (10K samples per target)',
                    'Optimized Random Forest parameters',
                    'Better class balancing with balanced_subsample'
                ]
            }

            os.makedirs('reports', exist_ok=True)
            timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
            report_file = f'reports/optimized_training_{timestamp}.json'

            with open(report_file, 'w') as f:
                json.dump(report, f, indent=2)

            logger.info(f"Optimized report saved to {report_file}")

        except Exception as e:
            logger.error(f"Error saving report: {e}")

    def run_optimized_training(self):
        """Run complete optimized training"""
        logger.info("Starting optimized model training...")

        try:
            # Load data
            stock_data_list = self.load_optimized_data()

            if not stock_data_list:
                logger.error("No valid stock data found")
                return False

            # Train model
            success, accuracy = self.train_optimized_model(stock_data_list)

            # Save report
            self.save_optimized_report(stock_data_list, accuracy)

            if success:
                logger.info(f"ðŸŽ‰ OPTIMIZED MODEL ACHIEVED {accuracy:.1%} ACCURACY! ðŸŽ‰")
                logger.info("Model ready for deployment!")
                return True
            elif accuracy >= 0.75:
                logger.info(f"Model achieved {accuracy:.1%} accuracy (above 75% threshold)")
                logger.info("Model saved and ready for use")
                return True
            else:
                logger.warning(f"Model achieved {accuracy:.1%} accuracy")
                return False

        except Exception as e:
            logger.error(f"Error during optimized training: {e}")
            return False

def main():
    """Main function"""
    trainer = OptimizedModelTrainer()
    success = trainer.run_optimized_training()

    if success:
        logger.info("Optimized training completed successfully!")
        sys.exit(0)
    else:
        logger.error("Optimized training failed!")
        sys.exit(1)

if __name__ == "__main__":
    main()
